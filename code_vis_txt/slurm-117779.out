Reading CSV..

Done Reading..

0it [00:00, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2936.32it/s]torch.Size([50, 3, 224, 224])
Starting VQA..

Extracting Embeddings..


/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
0it [00:30, ?it/s]
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/TUE/20210962/flava_vqa.py:174 in <module>                              â”‚
â”‚                                                                              â”‚
â”‚   171 â”‚   inputs["input_ids_masked"] = inputs["input_ids"].detach().clone()  â”‚
â”‚   172 â”‚   inputs["bool_masked_pos"] = torch.zeros_like(inputs["bool_masked_p â”‚
â”‚   173 â”‚   inputs = inputs.to('cuda')                                         â”‚
â”‚ â± 174 â”‚   outputs = flava_model(**inputs)                                    â”‚
â”‚   175 â”‚   mm_embedding = outputs.multimodal_masked_output.last_hidden_state. â”‚
â”‚   176 â”‚   mm_embeddings.append(mm_embedding)                                 â”‚
â”‚   177                                                                        â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:1859 in forward                 â”‚
â”‚                                                                              â”‚
â”‚   1856 â”‚   â”‚   â”‚   return_dict=True,                                         â”‚
â”‚   1857 â”‚   â”‚   )                                                             â”‚
â”‚   1858 â”‚   â”‚                                                                 â”‚
â”‚ â± 1859 â”‚   â”‚   flava_masked_output = self.flava(                             â”‚
â”‚   1860 â”‚   â”‚   â”‚   input_ids=input_ids_masked,                               â”‚
â”‚   1861 â”‚   â”‚   â”‚   pixel_values=pixel_values,                                â”‚
â”‚   1862 â”‚   â”‚   â”‚   attention_mask=attention_mask,                            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:1388 in forward                 â”‚
â”‚                                                                              â”‚
â”‚   1385 â”‚   â”‚   image_mm_projection = None                                    â”‚
â”‚   1386 â”‚   â”‚   image_output = None                                           â”‚
â”‚   1387 â”‚   â”‚   if pixel_values is not None:                                  â”‚
â”‚ â± 1388 â”‚   â”‚   â”‚   image_output = self.image_model(                          â”‚
â”‚   1389 â”‚   â”‚   â”‚   â”‚   pixel_values=pixel_values,                            â”‚
â”‚   1390 â”‚   â”‚   â”‚   â”‚   bool_masked_pos=bool_masked_pos,                      â”‚
â”‚   1391 â”‚   â”‚   â”‚   â”‚   attention_mask=image_attention_mask,                  â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:964 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    961 â”‚   â”‚   â”‚   pixel_values, bool_masked_pos=bool_masked_pos, interpolat â”‚
â”‚    962 â”‚   â”‚   )                                                             â”‚
â”‚    963 â”‚   â”‚                                                                 â”‚
â”‚ â±  964 â”‚   â”‚   encoder_outputs = self.encoder(                               â”‚
â”‚    965 â”‚   â”‚   â”‚   embedding_output,                                         â”‚
â”‚    966 â”‚   â”‚   â”‚   attention_mask=attention_mask,                            â”‚
â”‚    967 â”‚   â”‚   â”‚   head_mask=head_mask,                                      â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:681 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    678 â”‚   â”‚   â”‚   â”‚   â”‚   layer_head_mask,                                  â”‚
â”‚    679 â”‚   â”‚   â”‚   â”‚   )                                                     â”‚
â”‚    680 â”‚   â”‚   â”‚   else:                                                     â”‚
â”‚ â±  681 â”‚   â”‚   â”‚   â”‚   layer_outputs = layer_module(hidden_states, attention â”‚
â”‚    682 â”‚   â”‚   â”‚                                                             â”‚
â”‚    683 â”‚   â”‚   â”‚   hidden_states = layer_outputs[0]                          â”‚
â”‚    684                                                                       â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:631 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    628 â”‚   â”‚                                                                 â”‚
â”‚    629 â”‚   â”‚   # in ViT, layernorm is also applied after self-attention      â”‚
â”‚    630 â”‚   â”‚   layer_output = self.layernorm_after(hidden_states)            â”‚
â”‚ â±  631 â”‚   â”‚   layer_output = self.intermediate(layer_output)                â”‚
â”‚    632 â”‚   â”‚                                                                 â”‚
â”‚    633 â”‚   â”‚   # second residual connection is done here                     â”‚
â”‚    634 â”‚   â”‚   layer_output = self.output(layer_output, hidden_states)       â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:574 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    571 â”‚   def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:   â”‚
â”‚    572 â”‚   â”‚                                                                 â”‚
â”‚    573 â”‚   â”‚   hidden_states = self.dense(hidden_states)                     â”‚
â”‚ â±  574 â”‚   â”‚   hidden_states = self.intermediate_act_fn(hidden_states)       â”‚
â”‚    575 â”‚   â”‚                                                                 â”‚
â”‚    576 â”‚   â”‚   return hidden_states                                          â”‚
â”‚    577                                                                       â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/activations.py:57 in forward                                   â”‚
â”‚                                                                              â”‚
â”‚    54 â”‚   â”‚   return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0))) â”‚
â”‚    55 â”‚                                                                      â”‚
â”‚    56 â”‚   def forward(self, input: Tensor) -> Tensor:                        â”‚
â”‚ â±  57 â”‚   â”‚   return self.act(input)                                         â”‚
â”‚    58                                                                        â”‚
â”‚    59                                                                        â”‚
â”‚    60 class FastGELUActivation(nn.Module):                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
RuntimeError: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 10.92 GiB
total capacity; 9.91 GiB already allocated; 15.38 MiB free; 10.16 GiB reserved 
in total by PyTorch) If reserved memory is >> allocated memory try setting 
max_split_size_mb to avoid fragmentation.  See documentation for Memory 
Management and PYTORCH_CUDA_ALLOC_CONF
/var/spool/slurm/d/job117779/slurm_script: line 6: deactivate: No such file or directory
