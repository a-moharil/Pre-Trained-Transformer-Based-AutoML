Reading CSV..

Done Reading..

0it [00:00, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2776.32it/s]torch.Size([50, 3, 224, 224])
Starting VQA..

Extracting Embeddings..


/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
0it [00:23, ?it/s]
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/TUE/20210962/flava_vqa.py:173 in <module>                              â”‚
â”‚                                                                              â”‚
â”‚   170 â”‚   inputs["input_ids_masked"] = inputs["input_ids"].detach().clone()  â”‚
â”‚   171 â”‚   inputs["bool_masked_pos"] = torch.zeros_like(inputs["bool_masked_p â”‚
â”‚   172 â”‚   inputs = inputs.to('cuda')                                         â”‚
â”‚ â± 173 â”‚   outputs = flava_model(**inputs)                                    â”‚
â”‚   174 â”‚   mm_embedding = outputs.multimodal_masked_output.last_hidden_state. â”‚
â”‚   175 â”‚   mm_embeddings.append(mm_embedding)                                 â”‚
â”‚   176                                                                        â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:1859 in forward                 â”‚
â”‚                                                                              â”‚
â”‚   1856 â”‚   â”‚   â”‚   return_dict=True,                                         â”‚
â”‚   1857 â”‚   â”‚   )                                                             â”‚
â”‚   1858 â”‚   â”‚                                                                 â”‚
â”‚ â± 1859 â”‚   â”‚   flava_masked_output = self.flava(                             â”‚
â”‚   1860 â”‚   â”‚   â”‚   input_ids=input_ids_masked,                               â”‚
â”‚   1861 â”‚   â”‚   â”‚   pixel_values=pixel_values,                                â”‚
â”‚   1862 â”‚   â”‚   â”‚   attention_mask=attention_mask,                            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:1388 in forward                 â”‚
â”‚                                                                              â”‚
â”‚   1385 â”‚   â”‚   image_mm_projection = None                                    â”‚
â”‚   1386 â”‚   â”‚   image_output = None                                           â”‚
â”‚   1387 â”‚   â”‚   if pixel_values is not None:                                  â”‚
â”‚ â± 1388 â”‚   â”‚   â”‚   image_output = self.image_model(                          â”‚
â”‚   1389 â”‚   â”‚   â”‚   â”‚   pixel_values=pixel_values,                            â”‚
â”‚   1390 â”‚   â”‚   â”‚   â”‚   bool_masked_pos=bool_masked_pos,                      â”‚
â”‚   1391 â”‚   â”‚   â”‚   â”‚   attention_mask=image_attention_mask,                  â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:964 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    961 â”‚   â”‚   â”‚   pixel_values, bool_masked_pos=bool_masked_pos, interpolat â”‚
â”‚    962 â”‚   â”‚   )                                                             â”‚
â”‚    963 â”‚   â”‚                                                                 â”‚
â”‚ â±  964 â”‚   â”‚   encoder_outputs = self.encoder(                               â”‚
â”‚    965 â”‚   â”‚   â”‚   embedding_output,                                         â”‚
â”‚    966 â”‚   â”‚   â”‚   attention_mask=attention_mask,                            â”‚
â”‚    967 â”‚   â”‚   â”‚   head_mask=head_mask,                                      â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:681 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    678 â”‚   â”‚   â”‚   â”‚   â”‚   layer_head_mask,                                  â”‚
â”‚    679 â”‚   â”‚   â”‚   â”‚   )                                                     â”‚
â”‚    680 â”‚   â”‚   â”‚   else:                                                     â”‚
â”‚ â±  681 â”‚   â”‚   â”‚   â”‚   layer_outputs = layer_module(hidden_states, attention â”‚
â”‚    682 â”‚   â”‚   â”‚                                                             â”‚
â”‚    683 â”‚   â”‚   â”‚   hidden_states = layer_outputs[0]                          â”‚
â”‚    684                                                                       â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:617 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    614 â”‚   â”‚   head_mask: Optional[torch.Tensor] = None,                     â”‚
â”‚    615 â”‚   â”‚   output_attentions: bool = False,                              â”‚
â”‚    616 â”‚   ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor] â”‚
â”‚ â±  617 â”‚   â”‚   self_attention_outputs = self.attention(                      â”‚
â”‚    618 â”‚   â”‚   â”‚   self.layernorm_before(hidden_states),  # in ViT, layernor â”‚
â”‚    619 â”‚   â”‚   â”‚   attention_mask=attention_mask,                            â”‚
â”‚    620 â”‚   â”‚   â”‚   head_mask=head_mask,                                      â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:551 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    548 â”‚   â”‚   head_mask: Optional[torch.Tensor] = None,                     â”‚
â”‚    549 â”‚   â”‚   output_attentions: bool = False,                              â”‚
â”‚    550 â”‚   ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor] â”‚
â”‚ â±  551 â”‚   â”‚   self_outputs = self.attention(                                â”‚
â”‚    552 â”‚   â”‚   â”‚   hidden_states, attention_mask=attention_mask, head_mask=h â”‚
â”‚    553 â”‚   â”‚   )                                                             â”‚
â”‚    554                                                                       â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /torch/nn/modules/module.py:1130 in _call_impl                               â”‚
â”‚                                                                              â”‚
â”‚   1127 â”‚   â”‚   # this function, and just call forward.                       â”‚
â”‚   1128 â”‚   â”‚   if not (self._backward_hooks or self._forward_hooks or self._ â”‚
â”‚   1129 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚
â”‚ â± 1130 â”‚   â”‚   â”‚   return forward_call(*input, **kwargs)                     â”‚
â”‚   1131 â”‚   â”‚   # Do not call functions when jit is used                      â”‚
â”‚   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚
â”‚   1133 â”‚   â”‚   if self._backward_hooks or _global_backward_hooks:            â”‚
â”‚                                                                              â”‚
â”‚ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages â”‚
â”‚ /transformers/models/flava/modeling_flava.py:469 in forward                  â”‚
â”‚                                                                              â”‚
â”‚    466 â”‚   â”‚   query_layer = self.transpose_for_scores(mixed_query_layer)    â”‚
â”‚    467 â”‚   â”‚                                                                 â”‚
â”‚    468 â”‚   â”‚   # Take the dot product between "query" and "key" to get the r â”‚
â”‚ â±  469 â”‚   â”‚   attention_scores = torch.matmul(query_layer, key_layer.transp â”‚
â”‚    470 â”‚   â”‚                                                                 â”‚
â”‚    471 â”‚   â”‚   attention_scores = attention_scores / math.sqrt(self.attentio â”‚
â”‚    472 â”‚   â”‚   if attention_mask is not None:                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
RuntimeError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.92 GiB 
total capacity; 10.06 GiB already allocated; 27.38 MiB free; 10.15 GiB reserved 
in total by PyTorch) If reserved memory is >> allocated memory try setting 
max_split_size_mb to avoid fragmentation.  See documentation for Memory 
Management and PYTORCH_CUDA_ALLOC_CONF
/var/spool/slurm/d/job117777/slurm_script: line 6: deactivate: No such file or directory
