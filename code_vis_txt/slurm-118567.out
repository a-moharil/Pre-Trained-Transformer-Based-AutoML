Processing Df..

Done Processing Df..

0it [00:00, ?it/s]
  0%|          | 0/100 [00:00<?, ?it/s][A/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(

  1%|          | 1/100 [00:02<04:05,  2.48s/it][A
  2%|▏         | 2/100 [00:04<03:12,  1.97s/it][A
  3%|▎         | 3/100 [00:05<02:59,  1.85s/it][A
  4%|▍         | 4/100 [00:07<03:07,  1.95s/it][A
  5%|▌         | 5/100 [00:59<31:20, 19.80s/it][A
  6%|▌         | 6/100 [01:38<41:17, 26.36s/it][A
  7%|▋         | 7/100 [02:27<52:15, 33.71s/it][A
  8%|▊         | 8/100 [02:46<44:50, 29.25s/it][A
  9%|▉         | 9/100 [02:49<31:46, 20.95s/it][A
 10%|█         | 10/100 [02:54<23:46, 15.85s/it][A
 11%|█         | 11/100 [02:55<17:04, 11.51s/it][A
 12%|█▏        | 12/100 [02:57<12:24,  8.46s/it][A
 13%|█▎        | 13/100 [02:58<09:04,  6.26s/it][A
 14%|█▍        | 14/100 [03:04<08:42,  6.07s/it][A
 15%|█▌        | 15/100 [03:54<27:30, 19.42s/it][A
 16%|█▌        | 16/100 [04:46<40:47, 29.13s/it][A
 17%|█▋        | 17/100 [05:28<45:35, 32.96s/it][A
 18%|█▊        | 18/100 [05:47<39:24, 28.83s/it][A
 19%|█▉        | 19/100 [05:48<27:49, 20.61s/it][A
 20%|██        | 20/100 [05:49<19:33, 14.67s/it][A
 21%|██        | 21/100 [05:50<13:56, 10.59s/it][A
 22%|██▏       | 22/100 [05:51<09:59,  7.69s/it][A
 23%|██▎       | 23/100 [05:52<07:15,  5.65s/it][A
 24%|██▍       | 24/100 [05:57<07:02,  5.55s/it][A
 25%|██▌       | 25/100 [06:47<23:26, 18.76s/it][A
 26%|██▌       | 26/100 [07:39<35:21, 28.67s/it][A
 27%|██▋       | 27/100 [08:20<39:30, 32.48s/it][A
 28%|██▊       | 28/100 [09:03<42:43, 35.60s/it][A
 29%|██▉       | 29/100 [09:11<32:18, 27.30s/it][A
 30%|███       | 30/100 [09:15<23:42, 20.32s/it][A
 31%|███       | 31/100 [09:21<18:21, 15.97s/it][A
 32%|███▏      | 32/100 [09:23<13:26, 11.86s/it][A
 33%|███▎      | 33/100 [09:26<10:20,  9.27s/it][A
 34%|███▍      | 34/100 [09:28<07:44,  7.04s/it][A
 35%|███▌      | 35/100 [09:30<06:03,  5.59s/it][A
 36%|███▌      | 36/100 [09:33<04:59,  4.68s/it][A
 37%|███▋      | 37/100 [09:34<03:43,  3.54s/it][A
 38%|███▊      | 38/100 [09:35<02:52,  2.78s/it][A
 39%|███▉      | 39/100 [09:36<02:15,  2.21s/it][A
 40%|████      | 40/100 [09:36<01:47,  1.79s/it][A
 41%|████      | 41/100 [09:38<01:36,  1.64s/it][A
 42%|████▏     | 42/100 [09:39<01:22,  1.42s/it][A
 43%|████▎     | 43/100 [09:40<01:14,  1.31s/it][A
 44%|████▍     | 44/100 [09:41<01:11,  1.27s/it][A
 45%|████▌     | 45/100 [09:42<01:07,  1.24s/it][A
 46%|████▌     | 46/100 [09:43<01:03,  1.17s/it][A
 47%|████▋     | 47/100 [09:44<00:59,  1.13s/it][A
 48%|████▊     | 48/100 [09:55<03:26,  3.96s/it][A
 49%|████▉     | 49/100 [10:36<12:51, 15.13s/it][A
 50%|█████     | 50/100 [10:47<11:36, 13.93s/it][A
 51%|█████     | 51/100 [10:50<08:50, 10.84s/it][A
 52%|█████▏    | 52/100 [11:12<11:09, 13.95s/it][A
 53%|█████▎    | 53/100 [11:13<07:56, 10.14s/it][A
 54%|█████▍    | 54/100 [11:45<12:48, 16.70s/it][A
 55%|█████▌    | 55/100 [12:36<20:16, 27.04s/it][A
 56%|█████▌    | 56/100 [13:27<25:10, 34.34s/it][A
 57%|█████▋    | 57/100 [13:57<23:32, 32.85s/it][A
 58%|█████▊    | 58/100 [14:04<17:42, 25.29s/it][A
 59%|█████▉    | 59/100 [14:10<13:11, 19.30s/it][A
 60%|██████    | 60/100 [14:11<09:10, 13.77s/it][A
 61%|██████    | 61/100 [14:12<06:27,  9.93s/it][A
 62%|██████▏   | 62/100 [14:40<09:43, 15.36s/it][A
 63%|██████▎   | 63/100 [15:15<13:08, 21.32s/it][A
 64%|██████▍   | 64/100 [15:20<09:52, 16.47s/it][A
 65%|██████▌   | 65/100 [16:02<14:00, 24.02s/it][A
 66%|██████▌   | 66/100 [16:42<16:18, 28.77s/it][A
 67%|██████▋   | 67/100 [17:18<17:04, 31.05s/it][A
 68%|██████▊   | 68/100 [17:50<16:41, 31.28s/it][A
 69%|██████▉   | 69/100 [18:04<13:34, 26.29s/it][A
 70%|███████   | 70/100 [18:05<09:20, 18.68s/it][A
 71%|███████   | 71/100 [18:06<06:26, 13.34s/it][A
 72%|███████▏  | 72/100 [18:07<04:27,  9.57s/it][A
 73%|███████▎  | 73/100 [18:08<03:08,  6.96s/it][A
 74%|███████▍  | 74/100 [18:09<02:13,  5.15s/it][A
 75%|███████▌  | 75/100 [18:11<01:45,  4.20s/it][A
 76%|███████▌  | 76/100 [18:42<04:53, 12.23s/it][A
 77%|███████▋  | 77/100 [19:11<06:36, 17.24s/it][A
 78%|███████▊  | 78/100 [19:12<04:32, 12.40s/it][A
 79%|███████▉  | 79/100 [19:13<03:07,  8.95s/it][A
 80%|████████  | 80/100 [19:26<03:25, 10.26s/it][A
 81%|████████  | 81/100 [19:29<02:33,  8.08s/it][A
 82%|████████▏ | 82/100 [20:20<06:18, 21.01s/it][A
 83%|████████▎ | 83/100 [21:16<08:53, 31.38s/it][A
 84%|████████▍ | 84/100 [22:10<10:12, 38.29s/it][A
 85%|████████▌ | 85/100 [22:57<10:12, 40.83s/it][A
 86%|████████▌ | 86/100 [23:39<09:35, 41.14s/it][A
 87%|████████▋ | 87/100 [24:22<09:01, 41.64s/it][A
 88%|████████▊ | 88/100 [24:59<08:03, 40.31s/it][A
 89%|████████▉ | 89/100 [25:36<07:11, 39.25s/it][A
 90%|█████████ | 90/100 [26:01<05:52, 35.21s/it][A
 91%|█████████ | 91/100 [26:03<03:45, 25.07s/it][A
 92%|█████████▏| 92/100 [26:04<02:24, 18.07s/it][A
 93%|█████████▎| 93/100 [26:05<01:30, 12.96s/it][A
 94%|█████████▍| 94/100 [26:08<00:58,  9.68s/it][A
 95%|█████████▌| 95/100 [26:09<00:35,  7.18s/it][A
 96%|█████████▌| 96/100 [26:11<00:22,  5.58s/it][A
 97%|█████████▋| 97/100 [26:13<00:13,  4.53s/it][A
 98%|█████████▊| 98/100 [26:14<00:07,  3.67s/it][A
 99%|█████████▉| 99/100 [26:16<00:02,  2.88s/it][A
100%|██████████| 100/100 [26:16<00:00,  2.26s/it][A100%|██████████| 100/100 [26:16<00:00, 15.77s/it]
1it [26:21, 1581.06s/it]1it [26:21, 1581.50s/it]
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/TUE/20210962/flava_mod3_pf.py:97 in <module>                           │
│                                                                              │
│    94                                                                        │
│    95 # Getting tabular + text embeddings                                    │
│    96 text_img_embeddings = []                                               │
│ ❱  97 for idx, batch in tqdm(enumerate(train_loader)):                       │
│    98 │   img, text = batch                                                  │
│    99 │   converted_tensors = [torchvision.transforms.functional.to_pil_imag │
│   100 │   │   │   │   │   │    img]                                          │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /tqdm/std.py:1195 in __iter__                                                │
│                                                                              │
│   1192 │   │   time = self._time                                             │
│   1193 │   │                                                                 │
│   1194 │   │   try:                                                          │
│ ❱ 1195 │   │   │   for obj in iterable:                                      │
│   1196 │   │   │   │   yield obj                                             │
│   1197 │   │   │   │   # Update and possibly print the progressbar.          │
│   1198 │   │   │   │   # Note: does not call self.update(1) for speed optimi │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/dataloader.py:681 in __next__                              │
│                                                                              │
│    678 │   │   │   if self._sampler_iter is None:                            │
│    679 │   │   │   │   # TODO(https://github.com/pytorch/pytorch/issues/7675 │
│    680 │   │   │   │   self._reset()  # type: ignore[call-arg]               │
│ ❱  681 │   │   │   data = self._next_data()                                  │
│    682 │   │   │   self._num_yielded += 1                                    │
│    683 │   │   │   if self._dataset_kind == _DatasetKind.Iterable and \      │
│    684 │   │   │   │   │   self._IterableDataset_len_called is not None and  │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/dataloader.py:721 in _next_data                            │
│                                                                              │
│    718 │                                                                     │
│    719 │   def _next_data(self):                                             │
│    720 │   │   index = self._next_index()  # may raise StopIteration         │
│ ❱  721 │   │   data = self._dataset_fetcher.fetch(index)  # may raise StopIt │
│    722 │   │   if self._pin_memory:                                          │
│    723 │   │   │   data = _utils.pin_memory.pin_memory(data, self._pin_memor │
│    724 │   │   return data                                                   │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/_utils/fetch.py:52 in fetch                                │
│                                                                              │
│   49 │   │   │   data = [self.dataset[idx] for idx in possibly_batched_index │
│   50 │   │   else:                                                           │
│   51 │   │   │   data = self.dataset[possibly_batched_index]                 │
│ ❱ 52 │   │   return self.collate_fn(data)                                    │
│   53                                                                         │
│                                                                              │
│ /home/TUE/20210962/flava_mod3_pf.py:84 in collate_fn                         │
│                                                                              │
│    81 # defining a custom collate function                                   │
│    82 def collate_fn(batch):                                                 │
│    83 │   batch = list(filter(lambda x: x is not None, batch))               │
│ ❱  84 │   return torch.utils.data.dataloader.default_collate(batch)          │
│    85                                                                        │
│    86                                                                        │
│    87 batch_size = 100                                                       │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/_utils/collate.py:175 in default_collate                   │
│                                                                              │
│   172 │   │   transposed = list(zip(*batch))  # It may be accessed twice, so │
│   173 │   │                                                                  │
│   174 │   │   if isinstance(elem, tuple):                                    │
│ ❱ 175 │   │   │   return [default_collate(samples) for samples in transposed │
│   176 │   │   else:                                                          │
│   177 │   │   │   try:                                                       │
│   178 │   │   │   │   return elem_type([default_collate(samples) for samples │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/_utils/collate.py:175 in <listcomp>                        │
│                                                                              │
│   172 │   │   transposed = list(zip(*batch))  # It may be accessed twice, so │
│   173 │   │                                                                  │
│   174 │   │   if isinstance(elem, tuple):                                    │
│ ❱ 175 │   │   │   return [default_collate(samples) for samples in transposed │
│   176 │   │   else:                                                          │
│   177 │   │   │   try:                                                       │
│   178 │   │   │   │   return elem_type([default_collate(samples) for samples │
│                                                                              │
│ /home/TUE/20210962/miniconda3/envs/ambarish_base/lib/python3.8/site-packages │
│ /torch/utils/data/_utils/collate.py:153 in default_collate                   │
│                                                                              │
│   150 │   │   elif elem.shape == ():  # scalars                              │
│   151 │   │   │   return torch.as_tensor(batch)                              │
│   152 │   elif isinstance(elem, float):                                      │
│ ❱ 153 │   │   return torch.tensor(batch, dtype=torch.float64)                │
│   154 │   elif isinstance(elem, int):                                        │
│   155 │   │   return torch.tensor(batch)                                     │
│   156 │   elif isinstance(elem, string_classes):                             │
╰──────────────────────────────────────────────────────────────────────────────╯
TypeError: must be real number, not str
/var/spool/slurm/d/job118567/slurm_script: line 6: deactivate: No such file or directory
